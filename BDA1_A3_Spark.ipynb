{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdc7366-3834-4322-9f4f-f4f68070798b",
   "metadata": {},
   "source": [
    "# Big Data Analytics\n",
    "Praktikum Sommersemester 2023. <small>Version 1.0</small>\n",
    "\n",
    "**Aufgabe 3: Abfragen mit Apache Spark** \n",
    "\n",
    "Machen Sie sich mit Apache Spark vertraut. Bearbeiten Sie die Aufgaben indem Sie **Spark RDD** entsprechend transformieren. \n",
    "\n",
    "## Arbeitsanweisung\n",
    "Nutzen Sie die markierten Zellen im vorliegenden Notebook `BDA1_A3_Spark.ipynb` für Ihre Lösungen und laden Sie es in Ilias hoch. In den Zellen muss ausführbarer python code vorliegen. Die Ausgabe soll unterhalb der jeweiligen Zellen produziert werden.\n",
    "Liefern Sie auch aussagekräftiges Markdown zu Ihrem Code (Vorgehen, Quellen, etc) ab.\n",
    "\n",
    "**Hinweis**: Verwenden Sie für diese Aufgaben *nicht* Spark SQL und *keine* Dataframes. \n",
    "\n",
    "----\n",
    "\n",
    "## Vorbereitung\n",
    "* Verwenden Sie immer den vorgegeben Spark Master um Inkonsistenzen der python3 Versionen zwischen Worker und Client zu verhindern.\n",
    "* Ändern Sie nicht die SparkContext Konfiguration und beenden Sie bitte den SparkContext nachdem Sie die Bearbeitung beenden, um die Resourcen wieder frei zu geben! (`stop_sc1()`)\n",
    "* Stellen Sie sicher, dass `pyspark` installiert ist (pip install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee9da9a-4bdb-4a12-a6a8-8ae19a6b9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[K     |████████████████████████████▏   | 273.4 MB 116.9 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 310.8 MB 22 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 57.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=177e06ebb4cd33c3794e685e80071d1a294c8b5ff0d4ed057e0d6d9ce1fa3a85\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a92662-3732-4c4f-90ad-4e8a00e6df99",
   "metadata": {},
   "source": [
    "Für diese Aufgabe steht ein HDFS mit dem Namenode `namenode` unter Port `19000` bereit. \n",
    "\n",
    "Sie finden die folgenden Dateien darin:\n",
    "\n",
    "* **`/data/bda1/co2data.tsv`**<br>Datensatz von Messungen verschiedener CO<sub>2</sub>-Sensoren\n",
    "* **`/data/bda1/co2data_pm.tsv`**<br>Datensatz von Messungen verschiedener CO<sub>2</sub>-Sensoren mit Partikelmessung, pm ist die Partikeldichte, npm die Partikelanzahl bestimmter Größen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac4db35b-551e-4c73-84e3-db273149a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pprint import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(sys.version)  # python3 version\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "def stop_sc1():\n",
    "    \"\"\"stop spark context if exists\"\"\"\n",
    "    try:\n",
    "        sc1.stop()\n",
    "        print('Spark Context stopped')\n",
    "    except Exception as ex1:\n",
    "        print(f'No context stopped: {ex1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68760a31-332d-4d06-b86e-40a70a9f31a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spark Context erzeugen\n",
    "Die Session mit dem vorgegebenen Spark Master öffnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7494bc27-6e35-40be-bee9-493d94ea3e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context stopped\n"
     ]
    }
   ],
   "source": [
    "stop_sc1()\n",
    "\n",
    "# never ever change these lines!\n",
    "config = pyspark.SparkConf().setAll([('spark.executor.memory', '1g'), ('spark.executor.cores', '1'), ('spark.cores.max', '2'), ('spark.driver.memory','1g'), (\"spark.app.name\", os.environ['JUPYTERHUB_CLIENT_ID'])])\n",
    "sc1 = pyspark.SparkContext(master='spark://jupiter.bigdata.fh-aachen.de:17077', conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9304e7-5a61-4396-a45a-b0a0619f2f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aufgabe 3 a\n",
    "\n",
    "Untersuchen Sie die Sensordaten unter ``hdfs://namenode/data/bda1/*``. Beantworten Sie die folgenden Fragen, indem Sie jeweils geeignete RDD Operationen in Spark ausführen:\n",
    "\n",
    "1. Wieviele Messungen sind der Datenmenge `co2data.tsv` zu finden? Wieviele sind es in `co2data_pm.tsv` ?\n",
    "2. Wie lauten die Attributnamen der Datenmenge `co2data_pm.tsv` ? Geben Sie sie zeilenweise aus.\n",
    "3. Wieviele verschiedene Sensoren (angegeben im Feld _serial_number_) enhält die Datenmenge `co2data.tsv` ?  Beachten Sie den Hinweis unter 3a)\n",
    "4. Wieviele Datenpunkte je Sensor liegen vor in `co2data.tsv` ? Geben Sie sowohl Sensor als auch Anzahl aus und beachten Sie den Hinweis unter 3a)\n",
    "5. Was ist der höchste, und was der niedrigste Temperaturwert in der Datenmenge `co2data_pm.tsv` ? \n",
    "6. Was ist der durchschnittliche CO<sub>2</sub>-Wert je Sensor in der Datenmenge `co2data_pm.tsv` ? Runden Sie gerne auf drei Nachkommastellen und beachten Sie den Hinweis unter 3a)\n",
    "\n",
    "**Hinweis:**<br>Die Serial Number besteht aus drei Komponenten: `s_` als Präfix, die eindeutige MAC-Adresse des Sensors und einer Zahl. Die Eindeutigkeit wird nur über die MAC-Adresse bestimmt. Nutzen Sie die MAC-Adresse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1e3e0-a78c-4844-9666-b43ce3640cc8",
   "metadata": {},
   "source": [
    "Nachdem wir die Konfigurationen gesetzt und den Spark Kontext erzeugt haben, haben wir den Spark Kontext unter der Variable\n",
    "`sc1` gespeichert. Um die Datei unter `hdfs://namenode/data/bda1/*` zu lesen haben wir den Port `19000` wie oben in der Aufgabenstellung\n",
    "angekündigt definiert und einen Pfad: `hdfs://namenode:19000/data/bda1/co2data.tsv` bzw. `hdfs://namenode:19000/data/bda1/co2data_pm.tsv`\n",
    "erhalten. Mit der Funktion `textFile()` können wir nun die Dateien lesen und die Anzahl der Messungen mit der Operation `count() - 1` finden. \n",
    "Wir zielen 1 ab, um den Header zu überspringen und reine Daten zu zählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "000bc20a-de6c-4ecc-818b-b3059697f134",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a1"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Messungen in co2data.tsv: 14013884\n",
      "Anzahl der Messungen in co2data_pm.tsv: 14001337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 1\n",
    "read_co2data = sc1.textFile(\"hdfs://namenode:19000/data/bda1/co2data.tsv\")\n",
    "counter1 = read_co2data.count() - 1\n",
    "read_co2data_pm = sc1.textFile(\"hdfs://namenode:19000/data/bda1/co2data_pm.tsv\")\n",
    "counter2 = read_co2data_pm.count() - 1\n",
    "print(\"Anzahl der Messungen in co2data.tsv:\", counter1)\n",
    "print(\"Anzahl der Messungen in co2data_pm.tsv:\", counter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651efe27-b315-44a4-aada-af990d87dc88",
   "metadata": {},
   "source": [
    "Mit der Funktion `first()` bekommen wir die erste Zeile der Datenmenge und geben sie dann mit einer for Schleife aus, wobei wir \n",
    "jede Zeile der RDD durch einen Tab Delimiter `\\t` getrennt haben, um die einzelnen Felder zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a32cecc2-e1f2-49e9-abb0-a04db9ba0ee4",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"timestamp\"\n",
      "\"measurement_count\"\n",
      "\"version\"\n",
      "\"serial_number\"\n",
      "\"co2_ppm\"\n",
      "\"temperature_celsius\"\n",
      "\"relative_humidity_percent\"\n",
      "\"pm1\"\n",
      "\"pm2\"\n",
      "\"pm4\"\n",
      "\"pm10\"\n",
      "\"npm0\"\n",
      "\"npm1\"\n",
      "\"npm2\"\n",
      "\"npm4\"\n",
      "\"npm10\"\n",
      "\"ps\"\n"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 2\n",
    "attributes = read_co2data_pm.first()\n",
    "for attr in attributes.split(\"\\t\"):\n",
    "    print(attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b95ff-e173-4bfd-b9c9-929ea3dc074d",
   "metadata": {},
   "source": [
    "In dieser Aufgabe zählen wir mithilfe von mapReduce die Anzahl der eindeutigen Sensoren. Zunächst definieren wir die erste Zeile als Header,\n",
    "um es in der nächsten Zeile zu überspringen und Daten ohne Header zu erhalten. Danach erzeugen wir die Liste verschiedener Sensoren, wobei\n",
    "wir als Eigenschaft der Eindeutigkeit die Mac-Adresse nehmen, die zwischen der 3 und 15 Charakter liegt. Dann zählen wir die einzelnen\n",
    "Sensoren mithilfe von MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "450e5ba0-82a5-4fc2-a138-26e02d7cc1c7",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a3"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl verschiedenen Sensoren: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 3\n",
    "header = read_co2data.first()\n",
    "data_without_header = read_co2data.filter(lambda row: row != header)\n",
    "co2_data = data_without_header.map(lambda row: (row.split(\"\\t\")[1][3:15]))\n",
    "unique_s = co2_data.distinct()\n",
    "cnt = unique_s.map(lambda attr: (attr, 1)).reduceByKey(lambda x, y: x + y)\n",
    "count_unique_sens = cnt.count()\n",
    "print(\"Anzahl verschiedenen Sensoren:\", count_unique_sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c856d2-5920-495a-a2b6-db69d2568371",
   "metadata": {},
   "source": [
    "Hier zählen wir die Anzahl der eindeutigen Sensoren mit der Funktion `countByValue()` und geben diese Anzahl aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "591784c3-6a1a-4d96-b5d7-44184082f8ef",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a4"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor:  d8bfc014724e , Amount:  2103522\n",
      "Sensor:  10521c0202ab , Amount:  2064\n",
      "Sensor:  e8db84c5f771 , Amount:  1665530\n",
      "Sensor:  8caab57c3e19 , Amount:  1561046\n",
      "Sensor:  e8db84c5f33d , Amount:  2270696\n",
      "Sensor:  8caab57a6dd9 , Amount:  2781677\n",
      "Sensor:  10521c01cf19 , Amount:  385105\n",
      "Sensor:  d8bfc0147061 , Amount:  578457\n",
      "Sensor:  3c6105d3abae , Amount:  1533919\n",
      "Sensor:  8caab57cc961 , Amount:  1131868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 4\n",
    "cnt_byValue = co2_data.countByValue()\n",
    "for value, amt in cnt_byValue.items():\n",
    "    print(\"Sensor: \", value, \", Amount: \", amt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed437918-4570-4f04-9a35-4749180bcd6c",
   "metadata": {},
   "source": [
    "In dieser Aufgabe arbeiten wir der `co2data_pm`. Wir überspringen den Header(wollte schon oben definierte Variable nehmen, habe aber einen\n",
    "Fehler bekommen, daher nochmal definiert). Dann nehmen wir die Spalte mit den Temperaturwerten und stellen sicher, dass wir keinen `null`\n",
    "oder leeren Wert nehmen. Das realisieren wir mit der `filter()` Funktion. Danach wieder mit map ersetzen wir die Anführungszeichen bei\n",
    "Werten und casten, um eine Zahl zu erhalten. Schließlich haben wir eine Liste mit allen numerischen Temperaturwerten bekommen, woraus wir\n",
    "einen maximalen und minimalen Temperaturwert mit den Funktionen `max()` und `min()` bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43760148-b3ce-42ab-9012-1a8a5c3ce962",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a5"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximale Temperatur:  41.0 , Minimale Temperatur:  6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 5\n",
    "attributes = read_co2data_pm.first()\n",
    "data_pm_without_header = read_co2data_pm.filter(lambda row: row != attributes)\n",
    "co2_data_pm = data_pm_without_header.map(lambda row: row.split(\"\\t\")[5])\n",
    "data_without_null = co2_data_pm.filter(lambda row: row != '\"null\"' and row != '')\n",
    "numbers = data_without_null.map(lambda n: float(n.replace('\"', '')))\n",
    "max_temp = numbers.max()\n",
    "min_temp = numbers.min()\n",
    "print(\"Maximale Temperatur: \", max_temp, \", Minimale Temperatur: \", min_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735747f1-8eae-4285-aee3-816c666fe295",
   "metadata": {},
   "source": [
    "Wir arbeiten immer noch mit der Datei aus 3a 5. Nun brauchen wir ein RDD mit zwei Werten: Sensorname und durchschnittlicher CO2 Wert für\n",
    "diesen Sensor. Wir überspringen den Header wieder und erstellen ein RDD mit zwei Werten aus unserer Datei, hier haben wir noch den Sensor\n",
    "und sein CO2 Wert. Jetzt filtern wir wieder beide Spalten, um die `null` Werte zu entfernen und casten auf `float` den zweiten Wert aus\n",
    "der Tupel, was ein CO2 Wert ist. Und dann gruppieren wir die Keys, was in diesem Fall die Sensornamen sind und weisen diesen einzelnen Sensornamen \n",
    "den durchschnittlichen CO2 Wert zu, den wir durch Division der Summe aller CO2 Werte durch die Anzahl bekommen. Darüber hinaus runden wir\n",
    "die CO2 Zahlen bis auf 3. Nachkommazahl. Zum Schluss nutzen wir die Funktion `collect()`, um eine Liste der Tupeln mit Sensornamen und \n",
    "Durchschnittswerten zu bekommen und geben die Werte aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ab16b6a-822a-42df-9e73-10715d8b73f6",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a6"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor:  e8db84c5fc6a , Avg_CO2_Wert:  554.979\n",
      "Sensor:  308398a2a1f6 , Avg_CO2_Wert:  657.518\n",
      "Sensor:  8caab57c9751 , Avg_CO2_Wert:  519.263\n",
      "Sensor:  8caab57cc813 , Avg_CO2_Wert:  547.52\n",
      "Sensor:  8caab57d01da , Avg_CO2_Wert:  574.967\n",
      "Sensor:  308398a2ddcb , Avg_CO2_Wert:  481.186\n",
      "Sensor:  3c6105d3908f , Avg_CO2_Wert:  730.933\n",
      "Sensor:  8caab57cbb52 , Avg_CO2_Wert:  496.846\n",
      "Sensor:  a848fac03782 , Avg_CO2_Wert:  549.538\n",
      "Sensor:  ac0bfbd6547d , Avg_CO2_Wert:  486.608\n",
      "Sensor:  308398b5b69c , Avg_CO2_Wert:  1058.714\n",
      "Sensor:  308398a2fb52 , Avg_CO2_Wert:  541.715\n",
      "Sensor:  308398a2f790 , Avg_CO2_Wert:  688.314\n",
      "Sensor:  ac0bfbd64321 , Avg_CO2_Wert:  580.367\n",
      "Sensor:  308398b595c0 , Avg_CO2_Wert:  531.905\n",
      "Sensor:  e8db84c62ab4 , Avg_CO2_Wert:  560.562\n",
      "Sensor:  3c6105d381e8 , Avg_CO2_Wert:  546.007\n",
      "Sensor:  e8db84c5f33d , Avg_CO2_Wert:  613.406\n",
      "Sensor:  e8db84c5f771 , Avg_CO2_Wert:  960.3\n",
      "Sensor:  ac0bfbd85271 , Avg_CO2_Wert:  573.868\n",
      "Sensor:  3c6105d467fd , Avg_CO2_Wert:  640.568\n",
      "Sensor:  8caab57d0593 , Avg_CO2_Wert:  671.539\n",
      "Sensor:  308398b61fd3 , Avg_CO2_Wert:  860.563\n",
      "Sensor:  308398b651ee , Avg_CO2_Wert:  676.252\n",
      "Sensor:  e8db84c60450 , Avg_CO2_Wert:  603.941\n",
      "Sensor:  4091514f0284 , Avg_CO2_Wert:  896.765\n",
      "Sensor:  30839882dbce , Avg_CO2_Wert:  596.058\n",
      "Sensor:  308398a2000e , Avg_CO2_Wert:  653.749\n",
      "Sensor:  308398a26607 , Avg_CO2_Wert:  573.242\n",
      "Sensor:  308398824a5e , Avg_CO2_Wert:  532.384\n",
      "Sensor:  308398b552c4 , Avg_CO2_Wert:  556.71\n",
      "Sensor:  308398b54e4c , Avg_CO2_Wert:  704.048\n",
      "Sensor:  3c6105d4188d , Avg_CO2_Wert:  594.736\n",
      "Sensor:  308398b60c74 , Avg_CO2_Wert:  501.695\n",
      "Sensor:  10521c0202ab , Avg_CO2_Wert:  521.268\n",
      "Sensor:  ac0bfbd71044 , Avg_CO2_Wert:  524.589\n",
      "Sensor:  3c6105d389cc , Avg_CO2_Wert:  736.777\n",
      "Sensor:  8caab57b0e22 , Avg_CO2_Wert:  767.415\n",
      "Sensor:  308398a2a0e4 , Avg_CO2_Wert:  586.763\n",
      "Sensor:  3c6105d49d8d , Avg_CO2_Wert:  513.605\n",
      "Sensor:  30839882fcc8 , Avg_CO2_Wert:  843.63\n",
      "Sensor:  8caab57cfb10 , Avg_CO2_Wert:  521.988\n",
      "Sensor:  40915150b550 , Avg_CO2_Wert:  1174.667\n",
      "Sensor:  ac0bfbd857fb , Avg_CO2_Wert:  446.86\n",
      "Sensor:  8caab57cafa8 , Avg_CO2_Wert:  541.866\n",
      "Sensor:  a848fac03ff8 , Avg_CO2_Wert:  571.832\n",
      "Sensor:  308398a2a8b6 , Avg_CO2_Wert:  899.723\n",
      "Sensor:  3c6105d36eb0 , Avg_CO2_Wert:  722.079\n",
      "Sensor:  8caab57c3e19 , Avg_CO2_Wert:  733.523\n",
      "Sensor:  3c6105cffb3b , Avg_CO2_Wert:  514.215\n",
      "Sensor:  308398b6157d , Avg_CO2_Wert:  930.366\n",
      "Sensor:  3c6105d3abae , Avg_CO2_Wert:  746.999\n",
      "Sensor:  308398805c0b , Avg_CO2_Wert:  551.013\n",
      "Sensor:  3c6105d34ddc , Avg_CO2_Wert:  562.284\n",
      "Sensor:  a848fac0a9df , Avg_CO2_Wert:  694.491\n",
      "Sensor:  3c6105d45469 , Avg_CO2_Wert:  571.067\n",
      "Sensor:  8caab57ccaaf , Avg_CO2_Wert:  765.171\n",
      "Sensor:  3c6105d414f3 , Avg_CO2_Wert:  565.769\n",
      "Sensor:  3083988004ef , Avg_CO2_Wert:  623.578\n",
      "Sensor:  8caab57c89c3 , Avg_CO2_Wert:  766.469\n",
      "Sensor:  308398b620a0 , Avg_CO2_Wert:  853.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 6\n",
    "attributes = read_co2data_pm.first()\n",
    "data_pm_without_header = read_co2data_pm.filter(lambda row: row != attributes)\n",
    "co2_data_pm_values = data_pm_without_header.map(lambda sensor: (sensor.split(\"\\t\")[3][3:15], sensor.split(\"\\t\")[4]))\n",
    "data_without_null = co2_data_pm_values.filter(lambda row: row[0] != '\"null\"' and row[1] != '\"null\"')\n",
    "numbers_pm = data_without_null.map(lambda n: (n[0], float(n[1].replace('\"',''))))\n",
    "avg_numbers = numbers_pm.groupByKey().mapValues(lambda num: sum(num) / len(num))\n",
    "round_numbers = avg_numbers.mapValues(lambda num: round(num, 3))\n",
    "num_list = round_numbers.collect()\n",
    "for sensor, co2_val in num_list:\n",
    "    print(\"Sensor: \", sensor, \", Avg_CO2_Wert: \", co2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270679d-cf30-40f2-abad-e2d2c08302b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aufgabe 3 b\n",
    "\n",
    "Machen Sie Aussagen in Markdown zu den beiden Datenmengen, nachdem Sie sich die Information aus Spark gezogen haben.\n",
    "\n",
    "* Kommen Sensoren (MAC-Adresse in Serial Number) in beiden Datenmengen vor? Wenn ja, welche?\n",
    "* Anhand welchen Attributs können Sie auf vorhandene Werte in den Attributen `pm*` und `npm*` filtern? Geben Sie ein Beispiel.\n",
    "* Enthält eine Datenmenge \"fehlerhafte\" CO<sub>2</sub>-Messungen? Wenn ja, wieviele Messungen sind betroffen? Fehlerhaft ist ein Wert `null`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320f7ab-d4a9-4afd-a76b-8500b81d89dd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "In dieser Aufgabe haben wir zwei Listen der verschiedenen Sensoren aus `co2data` und `co2data_pm` erzeugt. Um die Duplikate aus den beiden\n",
    "Listen zu bekommen, haben wir die Funktion `intersection()` benutzt und mit `collect()` eine Liste der in beiden RDDs vorkommenden Sensoren\n",
    "ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "033579b2-cb33-43f3-992f-024270b52baa",
   "metadata": {
    "tags": [
     "bda1_ex2_a",
     "ex2_a2"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folgende Sensoren kommen in beiden Datenmengen vor:  ['e8db84c5f33d', 'e8db84c5f771', '10521c0202ab', '8caab57c3e19', '3c6105d3abae']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keine Zellenvorgabe\n",
    "header1 = read_co2data.first()\n",
    "data_without_header_1 = read_co2data.filter(lambda row: row != header1)\n",
    "co2_data = data_without_header_1.map(lambda row: (row.split(\"\\t\")[1][3:15]))\n",
    "unique_s = co2_data.distinct()\n",
    "\n",
    "header_2 = read_co2data_pm.first()\n",
    "data_without_header_2 = read_co2data_pm.filter(lambda row: row != header_2)\n",
    "co2_data_pm = data_without_header_2.map(lambda row: (row.split(\"\\t\")[3][3:15]))\n",
    "unique_s_pm = co2_data_pm.distinct()\n",
    "\n",
    "duplicates = unique_s.intersection(unique_s_pm).collect()\n",
    "print(\"Folgende Sensoren kommen in beiden Datenmengen vor: \", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55f0b5-2d1f-4f36-be8c-d2b9e837cd09",
   "metadata": {},
   "source": [
    "Hier nehmen wir den RDD von co2_data_pm und nehmen z.B die 7 Spalte, die `pm1` ist. Wir filtern mit der Funktion `filter()` auf vorhandene\n",
    "Werte `is not None and p!= '\"null\"'`. Dann nehmen wir die ersten 10 Werte, weil die ganze Liste zu groß ist und geben die aus. \n",
    "Die weiteren pm und npm Spalten können analog mit entsprechendem Spaltenindex ausgewählt und ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7574c151-bb2b-4355-bc57-702d0ecdad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ersten 10 Werte von pm1 auf existierende gefiltert:  ['\"1.58\"', '\"2.12\"', '\"1.59\"', '\"2.15\"', '\"1.69\"', '\"2.1\"', '\"1.77\"', '\"2.01\"', '\"1.78\"', '\"1.93\"']\n"
     ]
    }
   ],
   "source": [
    "header = read_co2data_pm.first()\n",
    "data_without_header = read_co2data_pm.filter(lambda row: row != header)\n",
    "pm1 = data_without_header.map(lambda row: (row.split(\"\\t\")[7]))\n",
    "filter_pm1 = pm1.filter(lambda p: p is not None and p != '\"null\"').take(10)\n",
    "print(\"Die ersten 10 Werte von pm1 auf existierende gefiltert: \", filter_pm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a808e53-90bf-4cf6-aa90-160f883df14f",
   "metadata": {},
   "source": [
    "In der letzten Aufgabe haben wir die mit der `co2_data_pm` gearbeitet. Die RDD haben wir die `null` Werte gefiltert und sie dann\n",
    "mit `count()` gezählt. Zum Schluss haben wir die Anzahl der fehlerhaften Messungen mit `print()` ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e01c58f-7f79-42a3-be81-b7d92791de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der fehlerhaften Messungen:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "co2_numbers = data_without_header.map(lambda row: (row.split(\"\\t\")[4]))\n",
    "null_vals = co2_numbers.filter(lambda k: k == '\"null\"').count()\n",
    "print(\"Anzahl der fehlerhaften Messungen: \", null_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e499f787-05a8-49ab-9b7c-95302e4afaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context stopped\n"
     ]
    }
   ],
   "source": [
    "stop_sc1()  # always exit your spark context after work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d7d8a-b8f7-41f8-b083-21d0da9f79ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nützliche Links\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "* https://spark.apache.org/examples.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
